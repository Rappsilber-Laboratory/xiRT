
# xiRT options generated with xiRT v. 1.2.3+2.g84a5484
# options for the recurrent layer used in xiRT
# can usually be used with default values, except for type
LSTM:
  # activation parameters, leave as default unless you know what you are doing
  activation: tanh
  activity_regularization: l2
  activityregularizer_value: 0.001
  
  # option that activates the bidirectional layer to the used LSTM layer
  bidirectional: true
  
  # kernal regularization, leave as default
  kernel_regularization: l2
  kernelregularizer_value: 0.001
  lstm_bn: true
  
  # central layer parameters
  # increasing the values here will drastically increase runtime but might also improve results
  # usually, 1 and GRU (for CPUs) or CuDNNGRU (for GPUs) will deliver good performance
  nlayers: 1
  type: GRU
  units: 50
 
# dense parameters are used for the individual task subnetworks (e.g. RP, SCX, ...)
dense:
  # activation functions in the layers between the embedding and prediction layer
  # recommended to leave on defaults for most applications
  activation:
  - relu
  - relu
  - relu
  
  # boolean indicator if batch_normalization shoulde be used, leave on default
  # recommended to leave on defaults for most applications
  dense_bn:
  - true
  - true
  - true
  
  # dropout rate to use
  # recommended to leave on defaults for most applications
  dropout:
  - 0.1
  - 0.1
  - 0.1
  
  # regularization methods to use on the kernels, leave on defaults
  kernel_regularizer:
  - l2
  - l2
  - l2
  regularization:
  - true
  - true
  - true
  regularizer_value:
  - 0.001
  - 0.001
  - 0.001
  # size of the individual layers, defaults deliver good results. Changes here might need adjustments 
  # on dropout rates and other hyper-parameters
  neurons:
  - 300
  - 150
  - 50
  
  # int, number of layers to use. Note that all other parameters in the 'dense' section
  # must be adapted to the new number used in this variable
  nlayers: 3

# dimension of the embedding output
embedding:
  length: 50
 
# parameters influencing the learning 
learning:
  # numbers of samples to pass during a single iteration
  batch_size: 512
  # number of epochs to train
  epochs: 50
  # other tested/reasonable values for learning rate: 0.003, 0.001
  learningrate: 0.01
  verbose: 1
  # default optimizer, most tensorflow optimizers are implemented as well
  optimizer: adam
  
#!!!!!!!!!!!!!!!!!! most important parameters!!!!!!!!!!!!!!!
output:
  # task-parameters. Here the prefix hsax and rp are used to build and parameterize the
  # respective sub-networks (this prefix must also match the "predictions" section. 
  # each task needs to contain the sufixes: activation, column, dimension, loss, metric and weight.

  # They must be carefully adapted for each prediction task.
  # recommended to use sigmoid for fractions (SCX/hSAX) if ordinal regression method should be used
  hsax-activation: sigmoid
  # columne where the fraction RT is in the CSV input
  hsax-column: hsax_ordinal
  # the number of unique / distinct values (e.g. fractions)
  hsax-dimension: 10
  # must be binary_crossentropy for sigmoid activations
  hsax-loss: binary_crossentropy
  # must be mse
  hsax-metrics: mse
  # weight parameter to combine the loss of this task to any other defined task
  hsax-weight: 50
  
  # use linear for regression tasks (revesed phase)
  rp-activation: linear
  rp-column: rp
  # dimension is always 1 for regression
  rp-dimension: 1
  # loss and metrics should not be changed from mse
  rp-loss: mse
  rp-metrics: mse
  # again, a weight parameter that might need tuning for multi-task settings
  rp-weight: 1

# siames parameters
siamese:
  # set to True for crosslinks (default)
  use: True
  # define how to combine the outputs of the siamese layers, most tensorflow options are supported.
  # default value should be fine
  merge_type: add
  # add predictions for single peptides based on the crosslink model (default)
  single_predictions: True
callbacks:
  # for debugging and model storage
  # define which callbacks to use.
  # default values are fine here and should not be changed
  # options define the meta data that is written throughout the training process. The results can 
  # be find in the callback in the specified outdir
  check_point: True
  log_csv: True
  # early stopping callback
  early_stopping: True
  early_stopping_patience: 15
  tensor_board: False
  progressbar: True
  # reduce learning rate callback
  reduce_lr: True
  reduce_lr_factor: 0.5
  reduce_lr_patience: 15
predictions:
  # define the prediction tasks unambiguously as they appear in the output file; need to match
  # column labels defined in output
  # "continues" is reserved for regression problems e.g. reversed-phase chromatography here
  continues:
    - rp
  # fractions are reserved for classification or ordinal regression problems e.g. 
  # fractionation method that led to discrete fractions
  # use [] if no fraction prediction is desired
  fractions: []# simply write fractions: [] if no fraction prediction is desired
